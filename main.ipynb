{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42667643",
   "metadata": {},
   "source": [
    "First import the dataset with stream on, and a num of workers for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408dd683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoIHeads(\n",
      "  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "  (box_head): FastRCNNConvFCHead(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "  )\n",
      "  (box_predictor): FastRCNNPredictor(\n",
      "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
      "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
      "  )\n",
      ")\n",
      "{'id': 'v_yvTmIulkl7c_2', 'conversations': [{'from': 'human', 'value': '<image>\\nis the child indoors\\nAnswer the question using few words or phrase.'}, {'from': 'gpt', 'value': 'yes'}], 'data_source': '0_30_s_activitynetqa', 'video': 'ActivityNet-QA/activitynet/train/v1-3/train_val/v_yvTmIulkl7c.mp4'}\n"
     ]
    }
   ],
   "source": [
    "from visual_cues import *\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from loader import *\n",
    "model = FRCNN()\n",
    "\n",
    "\n",
    "data = LoadActivityNetDataset()\n",
    "\n",
    "print(data[:1][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
